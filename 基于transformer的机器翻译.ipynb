{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1998814\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3981\n",
      "    })\n",
      "})\n",
      "({'en': 'It was emphasized that the cost-sharing arrangements in Nairobi between the United Nations Centre for Human Settlements, the United Nations Office at Nairobi and UNEP needed to be clarified and that savings resulting from the restructuring of the Centre, in particular in connection with the transfer of certain functions to the United Nations Office at Nairobi, needed to be identified.', 'zh': '有人强调,联合国人类住区中心、联合国内罗毕办事处和环境规划署之间在内罗毕订立的费用分担协议需要加以澄清,而且该中心调整后产生的节余,尤其是因为把某些职能转移给联合国内罗毕办事处而产生的节余都需要加以查明。'}, {'en': 'Last week, the broadcast of period drama “Beauty Private Kitchen” was temporarily halted, and accidentally triggered heated debate about faked ratings of locally produced dramas.', 'zh': '上周，古装剧《美人私房菜》临时停播，意外引发了关于国产剧收视率造假的热烈讨论。'})\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载数据\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import spacy\n",
    "# 加载 WMT19 中英文翻译数据集\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"wmt19\", \"zh-en\")\n",
    "train_data = dataset[\"train\"][\"translation\"][:20000]\n",
    "valid_data = dataset[\"validation\"][\"translation\"][:1000]\n",
    "print(dataset)\n",
    "print((train_dataset[0], validation_dataset[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文词汇表长度: 16955，英文词汇表长度: 18128\n"
     ]
    }
   ],
   "source": [
    "# 2. 加载 Spacy 语言模型（用于分词）\n",
    "spacy_zh = spacy.load(\"zh_core_web_sm\")  # 中文分词\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")  # 英文分词\n",
    "\n",
    "\n",
    "# 句子分词函数\n",
    "def tokenize_zh(text):\n",
    "    return [tok.text for tok in spacy_zh.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "# 3. 设定数据集的语言（来源：中文，目标：英文）\n",
    "SRC_LANGUAGE = 'zh'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "\n",
    "# 4. 手动生成词汇表\n",
    "def build_vocab_manual(data, tokenizer, min_freq=1):\n",
    "    counter = Counter()\n",
    "\n",
    "    # 统计词频\n",
    "    for text in data:\n",
    "        tokens = tokenizer(text)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # 构建词汇表\n",
    "    vocab = {token: idx for idx, (token, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<unk>\"] = len(vocab)\n",
    "    vocab[\"<pad>\"] = len(vocab)\n",
    "    vocab[\"<sos>\"] = len(vocab)\n",
    "    vocab[\"<eos>\"] = len(vocab)\n",
    "\n",
    "    # 创建反向映射\n",
    "    idx_to_token = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "    return vocab, idx_to_token\n",
    "\n",
    "\n",
    "# 提取中文和英文的句子\n",
    "zh_texts = [item[SRC_LANGUAGE] for item in train_dataset]\n",
    "en_texts = [item[TGT_LANGUAGE] for item in train_dataset]\n",
    "\n",
    "# 构建词汇表\n",
    "vocab_zh, idx_to_token_zh = build_vocab_manual(zh_texts, tokenize_zh)\n",
    "vocab_en, idx_to_token_en = build_vocab_manual(en_texts, tokenize_en)\n",
    "\n",
    "print(f\"中文词汇表长度: {len(vocab_zh)}，英文词汇表长度: {len(vocab_en)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 5. 定义数据处理函数（文本转 ID）\n",
    "def numericalize(text, vocab, tokenizer):\n",
    "    return [vocab[\"<sos>\"]] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokenizer(text)] + [vocab[\"<eos>\"]]\n",
    "\n",
    "# 6. 填补空缺\n",
    "def pad_sequences(seq, vocab, max_length = 50):\n",
    "    return seq + [vocab['<pad>']] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length]\n",
    "\n",
    "\n",
    "# 7. 构建 Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer, max_length=50):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data[idx]['zh']\n",
    "        tgt_text = self.data[idx]['en']\n",
    "\n",
    "        # 转化成ids\n",
    "        src_ids = numericalize(src_text, self.src_vocab, self.src_tokenizer)\n",
    "        tgt_ids = numericalize(tgt_text, self.tgt_vocab, self.tgt_tokenizer)\n",
    "        # 填充序列\n",
    "        src_ids = pad_sequences(src_ids, self.src_vocab, self.max_length)\n",
    "        tgt_ids = pad_sequences(tgt_ids, self.tgt_vocab, self.max_length)\n",
    "\n",
    "        return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "\n",
    "# 7. 创建 DataLoader\n",
    "train_dataset = TranslationDataset(train_dataset, vocab_zh, vocab_en, tokenize_zh, tokenize_en)\n",
    "valid_dataset = TranslationDataset(validation_dataset, vocab_zh, vocab_en, tokenize_zh, tokenize_en)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda batch: batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=lambda batch: batch)\n",
    "print(\"数据加载成功!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer模型构建成功！\n"
     ]
    }
   ],
   "source": [
    "# 8. Transformer 模型\n",
    "# Transformer 模型\n",
    "import numpy as np\n",
    "class TransformerMT(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=256, num_heads=8, num_layers=6, ff_dim=512,\n",
    "                 dropout=0.1, max_seq_len=50):\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_dim)\n",
    "        self.max_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout = dropout\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # 启用 batch_first\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "        self.src_pad_idx = vocab_zh[\"<pad>\"]\n",
    "        self.tgt_pad_idx = vocab_en[\"<pad>\"]\n",
    "\n",
    "    def create_positional_encoding(self, embed_dim, max_len):\n",
    "        \"\"\"生成固定的 Positional Encoding\"\"\"\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0).to(device)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        return (src == self.src_pad_idx)  # 形状: [batch_size, seq_len]\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        seq_len = tgt.shape[1]\n",
    "        tgt_sub_mask = torch.tril(\n",
    "            torch.ones(seq_len, seq_len, dtype=torch.bool, device=tgt.device))  # [seq_len, seq_len]\n",
    "        return tgt_sub_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # [batch_size, seq_len, embed_dim]\n",
    "        src_position = self.create_positional_encoding(self.embed_dim, src.shape[1])\n",
    "        tgt_position = self.create_positional_encoding(self.embed_dim, tgt.shape[1])\n",
    "        src1 = self.src_embedding(src) + src_position\n",
    "        tgt1 = self.tgt_embedding(tgt) + tgt_position\n",
    "\n",
    "        src_mask = self.make_src_mask(src)  # [batch_size, seq_len]\n",
    "        tgt_mask = self.make_tgt_mask(tgt)  # [seq_len, seq_len]\n",
    "\n",
    "        output = self.transformer(src1, tgt1, src_key_padding_mask=src_mask, tgt_mask=tgt_mask)  # [batch_size, seq_len, embed_dim]\n",
    "        return self.fc_out(output)  # [batch_size, seq_len, tgt_vocab_size]\n",
    "\n",
    "\n",
    "print(\"Transformer模型构建成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现模型文件 ./model/Translate_transformer.pt，正在加载...\n",
      "模型加载完成\n",
      "\tBatch 125/625, Loss: 0.2532\n",
      "\tBatch 250/625, Loss: 0.2953\n",
      "\tBatch 375/625, Loss: 0.3702\n",
      "\tBatch 500/625, Loss: 0.3052\n",
      "\tBatch 625/625, Loss: 0.2601\n",
      "Epoch 1/1, Loss: 0.2757\n"
     ]
    }
   ],
   "source": [
    "# 9. 训练\n",
    "import os\n",
    "model_path = './model/Translate_transformer.pt'\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-4):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab_en[\"<pad>\"])\n",
    "    # 检查模型是否已经存在\n",
    "    if os.path.isfile(model_path):\n",
    "        print(f\"发现模型文件 {model_path}，正在加载...\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(\"模型加载完成\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for num, batch in enumerate(train_loader):\n",
    "            src, tgt = zip(*batch)\n",
    "            src = torch.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=vocab_zh[\"<pad>\"]).to(device)\n",
    "            tgt = torch.nn.utils.rnn.pad_sequence(tgt, batch_first=True, padding_value=vocab_en[\"<pad>\"]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt[:, :-1])  # 目标序列去掉最后一个 token\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (num+1) % 125 == 0:\n",
    "                print(f\"\\tBatch {num+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        print(translate(model, train_dataset[0]['zh']))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerMT(len(vocab_zh), len(vocab_en)).to(device)\n",
    "train_model(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现模型文件 ./model/Translate_transformer.pt，正在加载...\n",
      "模型加载完成\n",
      "They , the Secretary - Self - Self - Self - Self - Self - Self - Self - Self - Self - Self - Self - Self - up of the Secretary - up of the Secretary - up of the Secretary - up of the Secretary - up\n"
     ]
    }
   ],
   "source": [
    "# 10. 推理\n",
    "def translate(model, src_sentence):\n",
    "    model.eval()\n",
    "    print(f\"中文: {src_sentence}\")\n",
    "    src_ids = numericalize(src_sentence, vocab_zh, tokenize_zh)\n",
    "    src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    tgt_ids = [vocab_en[\"<sos>\"]]\n",
    "    for _ in range(50):\n",
    "        tgt_tensor = torch.tensor(tgt_ids).unsqueeze(0).to(device)\n",
    "        output = model(src_tensor, tgt_tensor)\n",
    "        next_word = output.argmax(dim=-1)[:, -1].item()\n",
    "        if next_word == vocab_en[\"<eos>\"]:\n",
    "            break\n",
    "        tgt_ids.append(next_word)\n",
    "\n",
    "    return \" \".join([idx_to_token_en[i] for i in tgt_ids[1:]])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerMT(len(vocab_zh), len(vocab_en)).to(device)\n",
    "# 检查模型是否已经存在\n",
    "if os.path.isfile(model_path):\n",
    "    print(f\"发现模型文件 {model_path}，正在加载...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"模型加载完成\")\n",
    "print(translate(model, train_dataset[0]['zh']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
