{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "# 加载 WMT19 中英文翻译数据集\n",
    "data_path = './data/wmt19-zh-en'\n",
    "dataset = load_dataset(data_path)\n",
    "\n",
    "\n",
    "# 1. 加载预训练模型和分词器\n",
    "# 确认CUDA是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定义自定义数据集\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, type, max_length=128, size=None):\n",
    "        self.dataset = dataset[type][:size]['translation'] if size else dataset[type]['translation']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        source_text = sample['zh']\n",
    "        target_text = sample['en']\n",
    "\n",
    "        # Tokenize source and target texts\n",
    "        source_encoding = self.tokenizer(source_text, max_length=self.max_length, padding=\"max_length\", truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, max_length=self.max_length, padding=\"max_length\", truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "\n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # 忽略填充部分\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. 创建数据集对象\n",
    "train_dataset = TranslationDataset(dataset, tokenizer, type='train', size=40000)\n",
    "eval_dataset = TranslationDataset(dataset, tokenizer, type='validation', size=100)\n",
    "\n",
    "# 3. 定义数据加载器\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8, pin_memory=False)\n",
    "\n",
    "# 4. 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,  # 使用混合精度训练以节省显存\n",
    "    fp16_opt_level=\"O1\",  # 混合精度优化级别\n",
    ")\n",
    "\n",
    "\n",
    "# 5. 使用Trainer API进行训练\n",
    "trainer = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 45:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>2.152803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>1.985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>1.926454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.518700</td>\n",
       "      <td>1.900353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>1.898530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存微调后的模型\n",
    "trainer.save_model(\"./model/fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实践BLEU评估\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def translate(text, tokenizer, model, max_length=128):\n",
    "    # 进行分词\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "\n",
    "    # 解码输出\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "    \n",
    "# 计算 BLEU 评分的函数\n",
    "def compute_bleu(references, hypothesis):\n",
    "    \"\"\"\n",
    "    计算 BLEU 评分\n",
    "    :param references: 参考翻译（list of lists）\n",
    "    :param hypothesis: 机器翻译结果（string）\n",
    "    :return: BLEU score\n",
    "    \"\"\"\n",
    "    reference_tokens = [nltk.word_tokenize(ref) for ref in references]  # 参考翻译分词\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis)  # 模型翻译分词\n",
    "    smooth = SmoothingFunction().method1  # 进行平滑处理，防止极端情况\n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smooth)\n",
    "\n",
    "\n",
    "def Bleu_score(references, translations):\n",
    "    bleu_scores = []\n",
    "    for reference, translation in zip(references, translations):\n",
    "        bleu_score = compute_bleu([reference], translation)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)  # 计算 BLEU 平均分\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 指定已保存的模型路径\n",
    "model_path = \"./model/fine-tuned-model\"\n",
    "\n",
    "# 2. 加载保存的模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源文本:上周，古装剧《美人私房菜》临时停播，意外引发了关于国产剧收视率造假的热烈讨论。\n",
      "参考翻译:Last week, the broadcast of period drama “Beauty Private Kitchen” was temporarily halted, and accidentally triggered heated debate about faked ratings of locally produced dramas.\n",
      "模板翻译:The present cease of the present presence of the present drafting, United States private level, rapidly briefed up a heavy discussion on the fallowing rates of national products.\n",
      "源文本:民权团体针对密苏里州发出旅行警告\n",
      "参考翻译:Civil rights group issues travel warning for Missouri\n",
      "模板翻译:Civilian rights groups issued travel warnings against Misouri\n",
      "源文本:由于密苏里州的歧视性政策和种族主义袭击，美国有色人种促进协会 (NAACP) 向准备前往密苏里州出游的有色人群发出旅行警告。\n",
      "参考翻译:The National Association for the Advancement of Colored People has put out an alert for people of color traveling to Missouri because of the state's discriminatory policies and racist attacks.\n",
      "模板翻译:As a result of discriminated policies and racism attacks in Misouri States, the American Association for the Promotion of Cross Humans (NAACP) is providing travel warnings to those campaigns prepared to travel to Misouri.\n",
      "源文本:“2017 年 8 月 28 日生效的 NAACP 密苏里州旅行咨询中呼吁，因近期密苏里州发生了一系列可疑的种族性事件，所有非裔美籍旅行者、游客以及密苏里州人在密苏里州旅行时应特别注意并采取极其谨慎的态度，特此告知，”该团体的声明宣称。\n",
      "参考翻译:\"The NAACP Travel Advisory for the state of Missouri, effective through August 28th, 2017, calls for African American travelers, visitors and Missourians to pay special attention and exercise extreme caution when traveling throughout the state given the series of questionable, race-based incidents occurring statewide recently, and noted therein,\" the group's statement reads.\n",
      "模板翻译:“In the NAACP Misouri travel consultative, which entered into force on 28 August 2017, an appeal was made to all African American travelers, visitors and Misouri travelers to attend special attention and take a very cautious cauticity in traveling in Misouri, as a result of a ser\n",
      "源文本:NAACP 指出，最近通过的一项密苏里州法律使得人们更难赢得歧视诉讼，该州执法也一定程度上针对少数群体，这些现象促使该组织发布了旅行警告。\n",
      "参考翻译:A recent Missouri law making it harder for people to win discrimination lawsuits, as well as the state's law enforcement disproportionately targeting minorities prompted the group to issue the travel alert, the NAACP said.\n",
      "模板翻译:NAACP noted that the recently adopted Misouri legislation has made it more difficult to accelerate discrimination proceeds and that the law enforcement in the State has been applicable to minority groups to some extent, which has promoted the organization to issue travel warnings.\n",
      "BLEU 平均分: 0.04144144202550121\n"
     ]
    }
   ],
   "source": [
    "# 开始验证测试集\n",
    "import torch\n",
    "test_sentences = dataset['validation']['translation'][:5]\n",
    "references = []\n",
    "translations = []\n",
    "for sample in test_sentences:\n",
    "    references.append(sample['en'])\n",
    "    translated = translate(sample['zh'], tokenizer, model)  # 通过模型翻译\n",
    "    translations.append(translated)\n",
    "    print(f\"源文本:{sample['zh']}\")\n",
    "    print(f\"参考翻译:{sample['en']}\")\n",
    "    print(f\"模板翻译:{translated}\")\n",
    "\n",
    "avg_bleu = Bleu_score(references, translations)  # 计算 BLEU 平均分\n",
    "print(f\"BLEU 平均分: {avg_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
