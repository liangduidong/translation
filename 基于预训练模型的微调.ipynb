{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "# åŠ è½½ WMT19 ä¸­è‹±æ–‡ç¿»è¯‘æ•°æ®é›†\n",
    "data_path = './data/wmt19-zh-en'\n",
    "dataset = load_dataset(data_path)\n",
    "\n",
    "\n",
    "# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "# ç¡®è®¤CUDAæ˜¯å¦å¯ç”¨\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å®šä¹‰è‡ªå®šä¹‰æ•°æ®é›†\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, type, max_length=128, size=None):\n",
    "        self.dataset = dataset[type][:size]['translation'] if size else dataset[type]['translation']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        source_text = sample['zh']\n",
    "        target_text = sample['en']\n",
    "\n",
    "        # Tokenize source and target texts\n",
    "        source_encoding = self.tokenizer(source_text, max_length=self.max_length, padding=\"max_length\", truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, max_length=self.max_length, padding=\"max_length\", truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "\n",
    "        labels = target_encoding[\"input_ids\"].squeeze()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # å¿½ç•¥å¡«å……éƒ¨åˆ†\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. åˆ›å»ºæ•°æ®é›†å¯¹è±¡\n",
    "train_dataset = TranslationDataset(dataset, tokenizer, type='train', size=40000)\n",
    "eval_dataset = TranslationDataset(dataset, tokenizer, type='validation', size=100)\n",
    "\n",
    "# 3. å®šä¹‰æ•°æ®åŠ è½½å™¨\n",
    "# æ•°æ®åŠ è½½å™¨\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8, pin_memory=False)\n",
    "\n",
    "# 4. å®šä¹‰è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥èŠ‚çœæ˜¾å­˜\n",
    "    fp16_opt_level=\"O1\",  # æ··åˆç²¾åº¦ä¼˜åŒ–çº§åˆ«\n",
    ")\n",
    "\n",
    "\n",
    "# 5. ä½¿ç”¨Trainer APIè¿›è¡Œè®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 45:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>2.152803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>1.985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>1.926454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.518700</td>\n",
       "      <td>1.900353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>1.898530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\voice_clone\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
    "trainer.save_model(\"./model/fine-tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®è·µBLEUè¯„ä¼°\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def translate(text, tokenizer, model, max_length=128):\n",
    "    # è¿›è¡Œåˆ†è¯\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "    # æ¨¡å‹æ¨ç†\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "\n",
    "    # è§£ç è¾“å‡º\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "    \n",
    "# è®¡ç®— BLEU è¯„åˆ†çš„å‡½æ•°\n",
    "def compute_bleu(references, hypothesis):\n",
    "    \"\"\"\n",
    "    è®¡ç®— BLEU è¯„åˆ†\n",
    "    :param references: å‚è€ƒç¿»è¯‘ï¼ˆlist of listsï¼‰\n",
    "    :param hypothesis: æœºå™¨ç¿»è¯‘ç»“æœï¼ˆstringï¼‰\n",
    "    :return: BLEU score\n",
    "    \"\"\"\n",
    "    reference_tokens = [nltk.word_tokenize(ref) for ref in references]  # å‚è€ƒç¿»è¯‘åˆ†è¯\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis)  # æ¨¡å‹ç¿»è¯‘åˆ†è¯\n",
    "    smooth = SmoothingFunction().method1  # è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œé˜²æ­¢æç«¯æƒ…å†µ\n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smooth)\n",
    "\n",
    "\n",
    "def Bleu_score(references, translations):\n",
    "    bleu_scores = []\n",
    "    for reference, translation in zip(references, translations):\n",
    "        bleu_score = compute_bleu([reference], translation)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)  # è®¡ç®— BLEU å¹³å‡åˆ†\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æŒ‡å®šå·²ä¿å­˜çš„æ¨¡å‹è·¯å¾„\n",
    "model_path = \"./model/fine-tuned-model\"\n",
    "\n",
    "# 2. åŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æºæ–‡æœ¬:ä¸Šå‘¨ï¼Œå¤è£…å‰§ã€Šç¾äººç§æˆ¿èœã€‹ä¸´æ—¶åœæ’­ï¼Œæ„å¤–å¼•å‘äº†å…³äºå›½äº§å‰§æ”¶è§†ç‡é€ å‡çš„çƒ­çƒˆè®¨è®ºã€‚\n",
      "å‚è€ƒç¿»è¯‘:Last week, the broadcast of period drama â€œBeauty Private Kitchenâ€ was temporarily halted, and accidentally triggered heated debate about faked ratings of locally produced dramas.\n",
      "æ¨¡æ¿ç¿»è¯‘:The present cease of the present presence of the present drafting, United States private level, rapidly briefed up a heavy discussion on the fallowing rates of national products.\n",
      "æºæ–‡æœ¬:æ°‘æƒå›¢ä½“é’ˆå¯¹å¯†è‹é‡Œå·å‘å‡ºæ—…è¡Œè­¦å‘Š\n",
      "å‚è€ƒç¿»è¯‘:Civil rights group issues travel warning for Missouri\n",
      "æ¨¡æ¿ç¿»è¯‘:Civilian rights groups issued travel warnings against Misouri\n",
      "æºæ–‡æœ¬:ç”±äºå¯†è‹é‡Œå·çš„æ­§è§†æ€§æ”¿ç­–å’Œç§æ—ä¸»ä¹‰è¢­å‡»ï¼Œç¾å›½æœ‰è‰²äººç§ä¿ƒè¿›åä¼š (NAACP) å‘å‡†å¤‡å‰å¾€å¯†è‹é‡Œå·å‡ºæ¸¸çš„æœ‰è‰²äººç¾¤å‘å‡ºæ—…è¡Œè­¦å‘Šã€‚\n",
      "å‚è€ƒç¿»è¯‘:The National Association for the Advancement of Colored People has put out an alert for people of color traveling to Missouri because of the state's discriminatory policies and racist attacks.\n",
      "æ¨¡æ¿ç¿»è¯‘:As a result of discriminated policies and racism attacks in Misouri States, the American Association for the Promotion of Cross Humans (NAACP) is providing travel warnings to those campaigns prepared to travel to Misouri.\n",
      "æºæ–‡æœ¬:â€œ2017 å¹´ 8 æœˆ 28 æ—¥ç”Ÿæ•ˆçš„ NAACP å¯†è‹é‡Œå·æ—…è¡Œå’¨è¯¢ä¸­å‘¼åï¼Œå› è¿‘æœŸå¯†è‹é‡Œå·å‘ç”Ÿäº†ä¸€ç³»åˆ—å¯ç–‘çš„ç§æ—æ€§äº‹ä»¶ï¼Œæ‰€æœ‰éè£”ç¾ç±æ—…è¡Œè€…ã€æ¸¸å®¢ä»¥åŠå¯†è‹é‡Œå·äººåœ¨å¯†è‹é‡Œå·æ—…è¡Œæ—¶åº”ç‰¹åˆ«æ³¨æ„å¹¶é‡‡å–æå…¶è°¨æ…çš„æ€åº¦ï¼Œç‰¹æ­¤å‘ŠçŸ¥ï¼Œâ€è¯¥å›¢ä½“çš„å£°æ˜å®£ç§°ã€‚\n",
      "å‚è€ƒç¿»è¯‘:\"The NAACP Travel Advisory for the state of Missouri, effective through August 28th, 2017, calls for African American travelers, visitors and Missourians to pay special attention and exercise extreme caution when traveling throughout the state given the series of questionable, race-based incidents occurring statewide recently, and noted therein,\" the group's statement reads.\n",
      "æ¨¡æ¿ç¿»è¯‘:â€œIn the NAACP Misouri travel consultative, which entered into force on 28 August 2017, an appeal was made to all African American travelers, visitors and Misouri travelers to attend special attention and take a very cautious cauticity in traveling in Misouri, as a result of a ser\n",
      "æºæ–‡æœ¬:NAACP æŒ‡å‡ºï¼Œæœ€è¿‘é€šè¿‡çš„ä¸€é¡¹å¯†è‹é‡Œå·æ³•å¾‹ä½¿å¾—äººä»¬æ›´éš¾èµ¢å¾—æ­§è§†è¯‰è®¼ï¼Œè¯¥å·æ‰§æ³•ä¹Ÿä¸€å®šç¨‹åº¦ä¸Šé’ˆå¯¹å°‘æ•°ç¾¤ä½“ï¼Œè¿™äº›ç°è±¡ä¿ƒä½¿è¯¥ç»„ç»‡å‘å¸ƒäº†æ—…è¡Œè­¦å‘Šã€‚\n",
      "å‚è€ƒç¿»è¯‘:A recent Missouri law making it harder for people to win discrimination lawsuits, as well as the state's law enforcement disproportionately targeting minorities prompted the group to issue the travel alert, the NAACP said.\n",
      "æ¨¡æ¿ç¿»è¯‘:NAACP noted that the recently adopted Misouri legislation has made it more difficult to accelerate discrimination proceeds and that the law enforcement in the State has been applicable to minority groups to some extent, which has promoted the organization to issue travel warnings.\n",
      "BLEU å¹³å‡åˆ†: 0.04144144202550121\n"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹éªŒè¯æµ‹è¯•é›†\n",
    "import torch\n",
    "test_sentences = dataset['validation']['translation'][:5]\n",
    "references = []\n",
    "translations = []\n",
    "for sample in test_sentences:\n",
    "    references.append(sample['en'])\n",
    "    translated = translate(sample['zh'], tokenizer, model)  # é€šè¿‡æ¨¡å‹ç¿»è¯‘\n",
    "    translations.append(translated)\n",
    "    print(f\"æºæ–‡æœ¬:{sample['zh']}\")\n",
    "    print(f\"å‚è€ƒç¿»è¯‘:{sample['en']}\")\n",
    "    print(f\"æ¨¡æ¿ç¿»è¯‘:{translated}\")\n",
    "\n",
    "avg_bleu = Bleu_score(references, translations)  # è®¡ç®— BLEU å¹³å‡åˆ†\n",
    "print(f\"BLEU å¹³å‡åˆ†: {avg_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
